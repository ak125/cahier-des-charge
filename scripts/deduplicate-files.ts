import * as fs from 'fs/promises';
import * as path from 'path';
import chalk from 'chalk';
import { createHash } from 'crypto';

// Charger la configuration
async function getConfig() {
  const configPath = path.resolve('./cahier_check.config.json');
  const configData = await fs.readFile(configPath, 'utf-8');
  return JSON.parse(configData);
}

// Interface pour repr√©senter un groupe de fichiers similaires
interface SimilarFilesGroup {
  baseFileName: string;
  files: {
    path: string;
    hash: string;
    size: number;
    modified: Date;
  }[];
}

// Interface pour les r√©sultats de d√©duplication
interface DeduplicationResult {
  type: string;
  duplicatesFound: number;
  filesRemoved: number;
  filesMerged: number;
  groups: SimilarFilesGroup[];
}

// Ex√©cute la d√©duplication des fichiers
async function deduplicateFiles(): Promise<void> {
  console.log(chalk.blue('üîç D√©marrage de la d√©duplication des fichiers...'));
  
  try {
    const config = await getConfig();
    const cahierPath = path.resolve(config.paths.cahier);
    
    // Trouver et d√©dupliciter les fichiers par type
    const mdResult = await deduplicateFilesByType(cahierPath, '.md');
    const jsonResult = await deduplicateFilesByType(cahierPath, '.json');
    const tsResult = await deduplicateFilesByType(cahierPath, '.ts');
    
    // Afficher les r√©sultats
    console.log(chalk.green('\n‚úÖ D√©duplication termin√©e!'));
    console.log(chalk.white(`üìä R√©sum√©:`));
    
    console.log(chalk.blue(`üìÑ Fichiers Markdown:`));
    printResult(mdResult);
    
    console.log(chalk.blue(`üîß Fichiers JSON:`));
    printResult(jsonResult);
    
    console.log(chalk.blue(`üíª Fichiers TypeScript:`));
    printResult(tsResult);
    
    // Cr√©er un rapport de d√©duplication
    await createDeduplicationReport(cahierPath, [mdResult, jsonResult, tsResult]);
    
  } catch (error) {
    console.error(chalk.red(`‚ùå Erreur lors de la d√©duplication: ${error.message}`));
    process.exit(1);
  }
}

// Affiche le r√©sultat pour un type de fichier
function printResult(result: DeduplicationResult): void {
  console.log(chalk.white(`  - Groupes de duplicats trouv√©s: ${result.groups.length}`));
  console.log(chalk.white(`  - Fichiers en double: ${result.duplicatesFound}`));
  console.log(chalk.white(`  - Fichiers supprim√©s: ${result.filesRemoved}`));
  console.log(chalk.white(`  - Fichiers fusionn√©s: ${result.filesMerged}`));
}

// D√©duplique les fichiers d'un type sp√©cifique
async function deduplicateFilesByType(directoryPath: string, extension: string): Promise<DeduplicationResult> {
  console.log(`üîç Recherche de doublons pour les fichiers ${extension}...`);
  
  try {
    // Lister tous les fichiers avec l'extension demand√©e
    const files = await fs.readdir(directoryPath);
    const matchingFiles = files.filter(file => file.endsWith(extension));
    
    console.log(`  Trouv√© ${matchingFiles.length} fichiers ${extension}`);
    
    // R√©sultat initial
    const result: DeduplicationResult = {
      type: extension,
      duplicatesFound: 0,
      filesRemoved: 0,
      filesMerged: 0,
      groups: []
    };
    
    if (matchingFiles.length === 0) {
      return result;
    }
    
    // Regrouper les fichiers par nom de base
    const fileGroups: Record<string, string[]> = {};
    
    for (const file of matchingFiles) {
      // Extraction du nom de base (sans num√©ro de version)
      let baseFileName = file.replace(extension, '');
      
      // Enlever les num√©ros de version
      baseFileName = baseFileName.replace(/\.v\d+(\.\w+)/g, '$1');
      
      if (!fileGroups[baseFileName]) {
        fileGroups[baseFileName] = [];
      }
      
      fileGroups[baseFileName].push(file);
    }
    
    // Identifier les groupes avec des duplicats
    for (const [baseFileName, group] of Object.entries(fileGroups)) {
      if (group.length > 1) {
        // Il y a des doublons potentiels
        result.duplicatesFound += group.length - 1;
        
        // Collecter des informations sur chaque fichier du groupe
        const filesInfo = await Promise.all(
          group.map(async (file) => {
            const filePath = path.join(directoryPath, file);
            const stat = await fs.stat(filePath);
            const content = await fs.readFile(filePath, 'utf8');
            const hash = createHash('md5').update(content).digest('hex');
            
            return {
              path: filePath,
              hash,
              size: stat.size,
              modified: stat.mtime
            };
          })
        );
        
        // Ajouter le groupe √† la liste des r√©sultats
        result.groups.push({
          baseFileName,
          files: filesInfo
        });
      }
    }
    
    // D√©duplicater les groupes identifi√©s
    for (const group of result.groups) {
      // Trier les fichiers par date de modification (le plus r√©cent d'abord)
      group.files.sort((a, b) => b.modified.getTime() - a.modified.getTime());
      
      // V√©rifier s'il y a des doublons exacts (m√™me hash)
      const uniqueHashes = new Set(group.files.map(file => file.hash));
      
      if (uniqueHashes.size < group.files.length) {
        // Il y a des doublons exacts (m√™me contenu)
        console.log(chalk.yellow(`üîÑ Traitement des doublons exacts pour ${group.baseFileName}${extension}...`));
        
        // Garder le fichier le plus r√©cent de chaque hash unique
        const keptFiles = new Map<string, string>();
        const filesToDelete: string[] = [];
        
        for (const file of group.files) {
          if (!keptFiles.has(file.hash)) {
            // C'est le premier fichier avec ce hash, on le garde
            keptFiles.set(file.hash, file.path);
          } else {
            // C'est un doublon, on le supprime
            filesToDelete.push(file.path);
          }
        }
        
        // Supprimer les doublons
        for (const filePath of filesToDelete) {
          try {
            await fs.unlink(filePath);
            console.log(chalk.green(`  ‚úÖ Supprim√©: ${path.basename(filePath)}`));
            result.filesRemoved++;
          } catch (error) {
            console.error(chalk.red(`  ‚ùå Erreur lors de la suppression de ${filePath}: ${error.message}`));
          }
        }
      } else if (group.files.length > 1 && extension === '.md') {
        // Tous les fichiers ont un contenu diff√©rent, mais m√™me nom de base
        // Pour les fichiers MD, on peut essayer de les fusionner
        console.log(chalk.blue(`üîÑ Tentative de fusion pour ${group.baseFileName}${extension}...`));
        
        // Fusionner les fichiers
        try {
          const mergedPath = group.files[0].path;
          const otherPaths = group.files.slice(1).map(f => f.path);
          
          await mergeMarkdownFiles(mergedPath, otherPaths);
          
          // Supprimer les fichiers fusionn√©s
          for (const filePath of otherPaths) {
            await fs.unlink(filePath);
            console.log(chalk.green(`  ‚úÖ Fusionn√© et supprim√©: ${path.basename(filePath)}`));
            result.filesRemoved++;
          }
          
          result.filesMerged++;
        } catch (error) {
          console.error(chalk.red(`  ‚ùå Erreur lors de la fusion des fichiers pour ${group.baseFileName}: ${error.message}`));
        }
      } else if (group.files.length > 1 && extension === '.json') {
        // Tous les fichiers ont un contenu diff√©rent, mais m√™me nom de base
        // Pour les fichiers JSON, on peut tenter une fusion intelligente
        console.log(chalk.blue(`üîÑ Tentative de fusion pour ${group.baseFileName}${extension}...`));
        
        try {
          const mergedPath = group.files[0].path;
          const otherPaths = group.files.slice(1).map(f => f.path);
          
          await mergeJsonFiles(mergedPath, otherPaths);
          
          // Supprimer les fichiers fusionn√©s
          for (const filePath of otherPaths) {
            await fs.unlink(filePath);
            console.log(chalk.green(`  ‚úÖ Fusionn√© et supprim√©: ${path.basename(filePath)}`));
            result.filesRemoved++;
          }
          
          result.filesMerged++;
        } catch (error) {
          console.error(chalk.red(`  ‚ùå Erreur lors de la fusion des fichiers pour ${group.baseFileName}: ${error.message}`));
        }
      }
    }
    
    return result;
  } catch (error) {
    console.error(`Erreur lors de la d√©duplication: ${error.message}`);
    return {
      type: extension,
      duplicatesFound: 0,
      filesRemoved: 0,
      filesMerged: 0,
      groups: []
    };
  }
}

// Fusionne plusieurs fichiers Markdown
async function mergeMarkdownFiles(targetPath: string, otherPaths: string[]): Promise<void> {
  // Lire le contenu du fichier cible
  let targetContent = await fs.readFile(targetPath, 'utf8');
  
  // Pour chaque fichier √† fusionner
  for (const filePath of otherPaths) {
    const content = await fs.readFile(filePath, 'utf8');
    
    // Extraire les sections du fichier source
    const sections = extractMarkdownSections(content);
    const targetSections = extractMarkdownSections(targetContent);
    
    // Fusionner les sections
    for (const [heading, text] of Object.entries(sections)) {
      if (!targetSections[heading]) {
        // Section absente dans le fichier cible, l'ajouter
        targetContent += `\n\n${heading}\n\n${text}`;
      } else if (targetSections[heading].length < text.length) {
        // La section du fichier source est plus d√©taill√©e, la remplacer
        const regex = new RegExp(`${escapeRegExp(heading)}\\s*\\n[\\s\\S]*?(?=\\n##|$)`, 'g');
        targetContent = targetContent.replace(regex, `${heading}\n\n${text}`);
      }
    }
  }
  
  // √âcrire le r√©sultat dans le fichier cible
  await fs.writeFile(targetPath, targetContent, 'utf8');
}

// Extrait les sections d'un fichier Markdown
function extractMarkdownSections(content: string): Record<string, string> {
  const sections: Record<string, string> = {};
  const lines = content.split('\n');
  
  let currentHeading: string | null = null;
  let currentContent: string[] = [];
  
  for (const line of lines) {
    if (line.startsWith('#')) {
      // C'est un nouveau titre
      if (currentHeading) {
        // Sauvegarder la section pr√©c√©dente
        sections[currentHeading] = currentContent.join('\n').trim();
      }
      
      currentHeading = line;
      currentContent = [];
    } else if (currentHeading) {
      // Ajouter la ligne √† la section en cours
      currentContent.push(line);
    }
  }
  
  // Sauvegarder la derni√®re section
  if (currentHeading) {
    sections[currentHeading] = currentContent.join('\n').trim();
  }
  
  return sections;
}

// Fusionne plusieurs fichiers JSON
async function mergeJsonFiles(targetPath: string, otherPaths: string[]): Promise<void> {
  // Lire le contenu du fichier cible
  const targetContent = JSON.parse(await fs.readFile(targetPath, 'utf8'));
  
  // Pour chaque fichier √† fusionner
  for (const filePath of otherPaths) {
    try {
      const content = JSON.parse(await fs.readFile(filePath, 'utf8'));
      
      // Fusionner les objets selon le type de fichier
      if (path.basename(targetPath).includes('.backlog.json')) {
        // Fusion sp√©cifique pour les backlogs
        mergeBacklogs(targetContent, content);
      } else if (path.basename(targetPath).includes('.impact_graph.json')) {
        // Fusion sp√©cifique pour les graphes d'impact
        mergeImpactGraphs(targetContent, content);
      } else {
        // Fusion g√©n√©rique pour les autres JSON
        deepMerge(targetContent, content);
      }
    } catch (error) {
      console.warn(chalk.yellow(`  ‚ö†Ô∏è Erreur lors de la lecture/parsing de ${filePath}, ignor√©: ${error.message}`));
    }
  }
  
  // √âcrire le r√©sultat dans le fichier cible
  await fs.writeFile(targetPath, JSON.stringify(targetContent, null, 2), 'utf8');
}

// Fusionne deux objets backlog
function mergeBacklogs(target: any, source: any): void {
  // Conserver les m√©tadonn√©es du backlog le plus r√©cent (target)
  // Fusionner les t√¢ches
  if (Array.isArray(target.tasks) && Array.isArray(source.tasks)) {
    // Cr√©er un ensemble des types de t√¢ches existantes
    const existingTaskTypes = new Set(target.tasks.map(t => t.type));
    
    // Ajouter uniquement les t√¢ches qui n'existent pas d√©j√†
    for (const task of source.tasks) {
      if (!existingTaskTypes.has(task.type)) {
        target.tasks.push(task);
        existingTaskTypes.add(task.type);
      }
    }
    
    // Mettre √† jour la priorit√© si n√©cessaire
    if (target.priority === undefined && source.priority !== undefined) {
      target.priority = source.priority;
    }
  }
}

// Fusionne deux graphes d'impact
function mergeImpactGraphs(target: any, source: any): void {
  // Fusionner les n≈ìuds
  if (Array.isArray(target.nodes) && Array.isArray(source.nodes)) {
    // Cr√©er un ensemble des n≈ìuds existants
    const existingNodes = new Set(target.nodes);
    
    // Ajouter uniquement les n≈ìuds qui n'existent pas d√©j√†
    for (const node of source.nodes) {
      if (!existingNodes.has(node)) {
        target.nodes.push(node);
        existingNodes.add(node);
      }
    }
  }
  
  // Fusionner les ar√™tes
  if (Array.isArray(target.edges) && Array.isArray(source.edges)) {
    // Convertir les ar√™tes en cha√Ænes pour la comparaison
    const existingEdges = new Set(
      target.edges.map(edge => Array.isArray(edge) ? edge.join(',') : JSON.stringify(edge))
    );
    
    // Ajouter uniquement les ar√™tes qui n'existent pas d√©j√†
    for (const edge of source.edges) {
      const edgeStr = Array.isArray(edge) ? edge.join(',') : JSON.stringify(edge);
      
      if (!existingEdges.has(edgeStr)) {
        target.edges.push(edge);
        existingEdges.add(edgeStr);
      }
    }
  }
}

// Fusionne profond√©ment deux objets
function deepMerge(target: any, source: any): void {
  for (const key in source) {
    if (source.hasOwnProperty(key)) {
      if (source[key] && typeof source[key] === 'object' && !Array.isArray(source[key])) {
        // C'est un objet, fusion r√©cursive
        if (!target[key]) target[key] = {};
        deepMerge(target[key], source[key]);
      } else if (Array.isArray(source[key])) {
        // C'est un tableau, fusionner en √©vitant les doublons
        if (!target[key]) target[key] = [];
        
        for (const item of source[key]) {
          // V√©rifier si l'√©l√©ment existe d√©j√† dans le tableau cible
          const itemStr = typeof item === 'object' ? JSON.stringify(item) : item;
          const exists = target[key].some(targetItem => {
            const targetItemStr = typeof targetItem === 'object' ? JSON.stringify(targetItem) : targetItem;
            return targetItemStr === itemStr;
          });
          
          if (!exists) {
            target[key].push(item);
          }
        }
      } else {
        // C'est une valeur simple, la conserver si elle n'existe pas d√©j√†
        if (target[key] === undefined) {
          target[key] = source[key];
        }
      }
    }
  }
}

// Cr√©e un rapport de d√©duplication
async function createDeduplicationReport(cahierPath: string, results: DeduplicationResult[]): Promise<void> {
  const logsDir = path.join(cahierPath, '../logs');
  
  // Cr√©er le r√©pertoire de logs si n√©cessaire
  try {
    await fs.mkdir(logsDir, { recursive: true });
  } catch (error) {
    // Ignorer les erreurs si le r√©pertoire existe d√©j√†
  }
  
  // G√©n√©rer le nom du fichier de rapport
  const timestamp = new Date().toISOString().replace(/:/g, '-').replace(/\./g, '-');
  const reportPath = path.join(logsDir, `deduplication-report-${timestamp}.md`);
  
  // G√©n√©rer le contenu du rapport
  let content = `# Rapport de d√©duplication des fichiers\n\n`;
  content += `*G√©n√©r√© le ${new Date().toLocaleString()}*\n\n`;
  
  // Ajouter un r√©sum√© global
  const totalDuplicates = results.reduce((sum, r) => sum + r.duplicatesFound, 0);
  const totalRemoved = results.reduce((sum, r) => sum + r.filesRemoved, 0);
  const totalMerged = results.reduce((sum, r) => sum + r.filesMerged, 0);
  
  content += `## üìä R√©sum√©\n\n`;
  content += `- **Fichiers en double d√©tect√©s**: ${totalDuplicates}\n`;
  content += `- **Fichiers supprim√©s**: ${totalRemoved}\n`;
  content += `- **Fichiers fusionn√©s**: ${totalMerged}\n\n`;
  
  // Ajouter le d√©tail par type de fichier
  for (const result of results) {
    content += `## Fichiers ${result.type}\n\n`;
    
    if (result.groups.length === 0) {
      content += `Aucun doublon d√©tect√©.\n\n`;
      continue;
    }
    
    content += `### Groupes de fichiers similaires\n\n`;
    
    for (const group of result.groups) {
      content += `#### ${group.baseFileName}${result.type}\n\n`;
      content += `| Fichier | Taille | Date de modification | Action |\n`;
      content += `|---------|--------|---------------------|--------|\n`;
      
      for (const file of group.files) {
        const fileName = path.basename(file.path);
        const fileSizeKB = (file.size / 1024).toFixed(2);
        const fileDate = file.modified.toLocaleString();
        const action = file === group.files[0] ? "Conserv√©" : "Supprim√©/Fusionn√©";
        
        content += `| ${fileName} | ${fileSizeKB} KB | ${fileDate} | ${action} |\n`;
      }
      
      content += `\n`;
    }
  }
  
  // √âcrire le rapport
  await fs.writeFile(reportPath, content, 'utf8');
  console.log(chalk.green(`‚úÖ Rapport de d√©duplication cr√©√©: ${reportPath}`));
}

// √âchappe les caract√®res sp√©ciaux pour RegExp
function escapeRegExp(string: string): string {
  return string.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
}

export default deduplicateFiles;

// Ex√©cuter si appel√© directement
if (require.main === module) {
  deduplicateFiles().catch(error => {
    console.error(`Erreur lors de la d√©duplication: ${error.message}`);
    process.exit(1);
  });
}
