/**
 * Service d'IA locale utilisant LangChain + Ollama pour remplacer OpenAI
 * 
 * Ce service permet d'utiliser des mod√®les d'IA locaux (DeepSeek-Coder, Mistral, etc.)
 * via Ollama, avec cache Redis pour optimiser les performances et r√©duire la charge.
 */

import { ChatOllama } from 'langchain/chat_models/ollama';
import { BaseLanguageModel } from 'langchain/base_language';
import { ChainValues, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate } from 'langchain/prompts';
import { LLMChain } from 'langchain/chains';
import Redis from 'ioredis';
import { createHash } from 'crypto';
import { Logger } from '@nestjs/common';

export interface LocalAiServiceConfig {
  redisUrl: string;
  ollamaUrl: string;
  model: string;
  cacheExpiry?: number;  // Dur√©e de validit√© du cache en secondes (3600 par d√©faut)
  useCache?: boolean;    // Activer/d√©sactiver le cache (true par d√©faut)
}

export interface AiServiceOptions {
  systemPrompt?: string;
  temperature?: number;
  maxTokens?: number;
  cacheKey?: string;     // Cl√© sp√©cifique pour ce prompt (optionnel)
}

/**
 * Service d'IA locale utilisant Ollama et LangChain
 */
export class LocalAiService {
  private redis: Redis;
  private llm: BaseLanguageModel;
  private config: LocalAiServiceConfig;
  private logger = new Logger('LocalAiService');
  
  constructor(config: LocalAiServiceConfig) {
    this.config = {
      ...config,
      cacheExpiry: config.cacheExpiry || 3600,
      useCache: config.useCache !== false
    };
    
    // Initialiser Redis
    this.redis = new Redis(this.config.redisUrl);
    
    // Initialiser le mod√®le Ollama
    this.llm = new ChatOllama({
      baseUrl: this.config.ollamaUrl,
      model: this.config.model,
      temperature: 0.2, // Temp√©rature plus basse pour les t√¢ches de code
    });
    
    this.logger.log(`üîå Service IA locale initialis√© avec le mod√®le ${this.config.model}`);
  }
  
  /**
   * G√©n√®re une r√©ponse √† partir d'un prompt en utilisant le mod√®le d'IA locale
   */
  async generate(
    prompt: string,
    options: AiServiceOptions = {}
  ): Promise<string> {
    try {
      // G√©n√©rer une cl√© de cache bas√©e sur le prompt et les options
      const cacheKey = options.cacheKey || this.generateCacheKey(prompt, options);
      
      // V√©rifier le cache si activ√©
      if (this.config.useCache) {
        const cached = await this.redis.get(cacheKey);
        if (cached) {
          this.logger.debug(`‚úÖ R√©sultat trouv√© dans le cache pour la cl√© ${cacheKey}`);
          return JSON.parse(cached);
        }
      }
      
      // Construire le template de prompt
      const chatPrompt = ChatPromptTemplate.fromPromptMessages([
        SystemMessagePromptTemplate.fromTemplate(
          options.systemPrompt || 
          "Vous √™tes un assistant d'IA expert en d√©veloppement logiciel, sp√©cialis√© dans la migration PHP vers Remix."
        ),
        HumanMessagePromptTemplate.fromTemplate("{prompt}")
      ]);
      
      // Cr√©er la cha√Æne LLM
      const chain = new LLMChain({
        llm: this.llm,
        prompt: chatPrompt,
      });
      
      // Invoquer la cha√Æne
      this.logger.debug(`üì§ Envoi du prompt au mod√®le ${this.config.model}`);
      const response: ChainValues = await chain.call({
        prompt: prompt,
      });
      
      // Extraire la r√©ponse
      const result = response.text || "";
      
      // Mettre en cache le r√©sultat si le cache est activ√©
      if (this.config.useCache) {
        await this.redis.set(cacheKey, JSON.stringify(result), 'EX', this.config.cacheExpiry);
        this.logger.debug(`üíæ R√©sultat mis en cache avec la cl√© ${cacheKey} (expiration: ${this.config.cacheExpiry}s)`);
      }
      
      return result;
    } catch (error: any) {
      this.logger.error(`‚ùå Erreur lors de la g√©n√©ration de l'IA: ${error.message}`);
      throw error;
    }
  }
  
  /**
   * G√©n√®re une r√©ponse √† partir d'un template de QA
   */
  async generateQA(
    question: string,
    context?: string,
    options: AiServiceOptions = {}
  ): Promise<string> {
    // Construire un prompt de QA
    let prompt = question;
    
    if (context) {
      prompt = `Contexte:
${context}

Question:
${question}

R√©ponse:`;
    }
    
    // Utiliser un syst√®me prompt sp√©cifique pour QA
    const qaOptions = {
      ...options,
      systemPrompt: options.systemPrompt || 
        "Vous √™tes un assistant d'IA expert qui r√©pond √† des questions en se basant uniquement sur le contexte fourni.",
      cacheKey: options.cacheKey || this.generateCacheKey(`qa:${question}`, options)
    };
    
    return this.generate(prompt, qaOptions);
  }
  
  /**
   * G√©n√®re une analyse de code avec des suggestions
   */
  async analyzeCode(
    code: string,
    languageName: string,
    requirements?: string,
    options: AiServiceOptions = {}
  ): Promise<string> {
    const prompt = `Code √† analyser (${languageName}):
\`\`\`${languageName}
${code}
\`\`\`

${requirements ? `Exigences/Contexte:
${requirements}

` : ''}Veuillez analyser ce code et fournir:
1. Un r√©sum√© de ce que fait le code
2. Les probl√®mes potentiels ou bugs
3. Des suggestions d'am√©lioration de la qualit√©
4. Des recommandations pour la migration vers Remix
`;
    
    // Utiliser un syst√®me prompt sp√©cifique pour l'analyse de code
    const codeOptions = {
      ...options,
      systemPrompt: options.systemPrompt || 
        "Vous √™tes un ing√©nieur logiciel expert sp√©cialis√© dans l'analyse de code et la migration de PHP vers Remix.",
      cacheKey: options.cacheKey || this.generateCacheKey(`code:${languageName}:${code.slice(0, 100)}`, options)
    };
    
    return this.generate(prompt, codeOptions);
  }
  
  /**
   * G√©n√®re une transformation de code PHP vers Remix
   */
  async transformPhpToRemix(
    phpCode: string,
    routeInfo?: string,
    options: AiServiceOptions = {}
  ): Promise<string> {
    const prompt = `Code PHP √† transformer:
\`\`\`php
${phpCode}
\`\`\`

${routeInfo ? `Informations sur la route:
${routeInfo}

` : ''}Veuillez transformer ce code PHP en composants Remix (TypeScript/React), en fournissant:
1. Le fichier route.tsx principal
2. Le loader.ts si n√©cessaire
3. L'action.ts si le code contient des formulaires ou mutations
4. Les types TypeScript pour les donn√©es
5. Des explications sur les changements importants
`;
    
    // Utiliser un syst√®me prompt sp√©cifique pour la transformation de code
    const transformOptions = {
      ...options,
      systemPrompt: options.systemPrompt || 
        "Vous √™tes un expert en migration de PHP vers Remix qui g√©n√®re du code TypeScript/React de haute qualit√©.",
      cacheKey: options.cacheKey || this.generateCacheKey(`transform:php:${phpCode.slice(0, 100)}`, options)
    };
    
    return this.generate(prompt, transformOptions);
  }
  
  /**
   * G√©n√®re une cl√© de cache unique pour un prompt
   */
  private generateCacheKey(prompt: string, options: AiServiceOptions = {}): string {
    // Cr√©er une cha√Æne qui inclut le prompt et les options pertinentes
    const dataToHash = JSON.stringify({
      prompt,
      model: this.config.model,
      systemPrompt: options.systemPrompt,
      temperature: options.temperature,
      maxTokens: options.maxTokens
    });
    
    // G√©n√©rer un hash SHA-256
    const hash = createHash('sha256').update(dataToHash).digest('hex');
    
    return `ai:cache:${this.config.model}:${hash}`;
  }
  
  /**
   * Invalide une entr√©e sp√©cifique du cache
   */
  async invalidateCache(cacheKey: string): Promise<boolean> {
    try {
      const result = await this.redis.del(cacheKey);
      return result > 0;
    } catch (error) {
      this.logger.error(`‚ùå Erreur lors de l'invalidation du cache: ${error}`);
      return false;
    }
  }
  
  /**
   * Vide tout le cache d'IA
   */
  async clearCache(): Promise<boolean> {
    try {
      const keys = await this.redis.keys('ai:cache:*');
      if (keys.length > 0) {
        await this.redis.del(...keys);
        this.logger.log(`üßπ Cache IA vid√© (${keys.length} entr√©es supprim√©es)`);
      }
      return true;
    } catch (error) {
      this.logger.error(`‚ùå Erreur lors du vidage du cache: ${error}`);
      return false;
    }
  }
  
  /**
   * Ferme les connexions
   */
  async close(): Promise<void> {
    await this.redis.quit();
    this.logger.log('üëã Service IA locale arr√™t√©');
  }
}

/**
 * Helper pour cr√©er une instance du service IA locale
 */
export function createLocalAiService(config: LocalAiServiceConfig): LocalAiService {
  return new LocalAiService(config);
}

// Ex√©cution autonome si appel√© directement
if (require.main === module) {
  const config: LocalAiServiceConfig = {
    redisUrl: process.env.REDIS_URL || 'redis://localhost:6379',
    ollamaUrl: process.env.OLLAMA_URL || 'http://localhost:11434',
    model: process.env.OLLAMA_MODEL || 'deepseek-coder',
    cacheExpiry: parseInt(process.env.CACHE_EXPIRY || '3600'),
    useCache: process.env.USE_CACHE !== 'false'
  };
  
  const prompt = process.argv[2] || "Expliquez la diff√©rence entre React et Remix";
  
  console.log(`üöÄ Test du service IA locale avec le mod√®le ${config.model}`);
  console.log(`üìã Prompt: ${prompt}`);
  
  const service = createLocalAiService(config);
  
  service.generate(prompt)
    .then(result => {
      console.log(`\n‚úÖ R√©sultat:\n${result}`);
      return service.close();
    })
    .catch(error => {
      console.error(`‚ùå Erreur: ${error.message}`);
      service.close();
      process.exit(1);
    });
}