#!/bin/bash
# create-pipeline.sh - Script pour cr√©er et ex√©cuter le pipeline de migration
# Date: 10 avril 2025

echo "üöÄ Cr√©ation du pipeline de migration..."

# V√©rifier si n8n est en cours d'ex√©cution
n8n_status=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:5678)

if [ "$n8n_status" != "200" ] && [ "$n8n_status" != "401" ]; then
  echo "‚ö†Ô∏è n8n ne semble pas √™tre en cours d'ex√©cution (code HTTP: $n8n_status)"
  echo "üîÑ D√©marrage de n8n..."
  
  # V√©rifier si docker-compose.n8n.yml existe
  if [ ! -f "docker-compose.n8n.yml" ]; then
    echo "‚ùå Le fichier docker-compose.n8n.yml n'existe pas. Cr√©ation d'un fichier par d√©faut..."
    
    cat > docker-compose.n8n.yml << 'EOF'
version: '3'

services:
  n8n:
    image: n8nio/n8n:0.236.0
    container_name: migration-n8n
    restart: always
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=cahier-des-charges-migrator
      - N8N_HOST=${N8N_HOST:-localhost}
      - N8N_PORT=${N8N_PORT:-5678}
      - N8N_PROTOCOL=${N8N_PROTOCOL:-http}
      - NODE_ENV=production
      - WEBHOOK_URL=http://${N8N_HOST:-localhost}:${N8N_PORT:-5678}/
      - EXECUTIONS_PROCESS=main
      - DB_TYPE=sqlite
      - DB_SQLITE_PATH=/tmp/n8n.db
      - N8N_PUSH_BACKEND=websocket
      - N8N_LOG_LEVEL=debug
      - NODE_TLS_REJECT_UNAUTHORIZED=0
      - GENERIC_TIMEZONE=Europe/Paris
      - N8N_SKIP_OWNERSHIP_CHECK=true
      - N8N_USER_FOLDER=/tmp/n8n
    command: n8n start
EOF
  fi
  
  # D√©marrer n8n
  docker-compose -f docker-compose.n8n.yml down 2>/dev/null
  docker-compose -f docker-compose.n8n.yml up -d
  
  # Attendre que n8n soit pr√™t
  echo "‚è≥ Attente du d√©marrage de n8n..."
  attempts=0
  max_attempts=30
  
  while [ $attempts -lt $max_attempts ]; do
    status=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:5678)
    if [ "$status" = "200" ] || [ "$status" = "401" ]; then
      echo "‚úÖ n8n est pr√™t!"
      break
    fi
    
    attempts=$((attempts+1))
    echo "‚è≥ Attente de n8n... ($attempts/$max_attempts)"
    sleep 2
  done
  
  if [ $attempts -eq $max_attempts ]; then
    echo "‚ùå n8n n'a pas d√©marr√© dans le temps imparti."
    exit 1
  fi
fi

# Cr√©ation du dossier reports si n√©cessaire
mkdir -p reports/analysis
mkdir -p reports/migration
mkdir -p reports/validation

echo "üìã Cr√©ation des workflows du pipeline..."

# Fonction pour nettoyer le JSON (supprimer les commentaires)
function clean_json {
  local input_file=$1
  local output_file=$2
  
  # Supprimer les commentaires de type //
  sed -e 's|//.*$||g' "$input_file" | \
  # Supprimer les commentaires multi-lignes /* */
  sed -e 's|/\*.*\*/||g' | \
  # Supprimer les lignes vides
  grep -v '^[[:space:]]*$' > "$output_file"
  
  # V√©rifier que le JSON est valide
  if ! jq . "$output_file" > /dev/null 2>&1; then
    echo "‚ö†Ô∏è Le fichier $output_file contient du JSON invalide. Correction..."
    # Si jq √©choue, essayer une approche plus simple
    cat "$input_file" | grep -v '//' > "$output_file"
  fi
}

# Nettoyer le fichier n8n.pipeline.json
clean_json "n8n.pipeline.json" "n8n.pipeline.clean.json"

# R√©cup√©rer les workflows depuis les fichiers config/*.n8n.json
config_files=$(find config -name "*.n8n.json" -type f)
for config_file in $config_files; do
  filename=$(basename "$config_file")
  output_file="workflows/${filename%.n8n.json}.json"
  mkdir -p workflows
  
  echo "üîÑ Traitement du fichier $config_file..."
  clean_json "$config_file" "$output_file"
done

# Cr√©er un script Node.js pour importer les workflows dans n8n
cat > import-workflows.js << 'EOF'
#!/usr/bin/env node

const fs = require('fs');
const path = require('path');
const https = require('https');
const http = require('http');

// Configuration
const N8N_HOST = process.env.N8N_HOST || 'localhost';
const N8N_PORT = process.env.N8N_PORT || '5678';
const N8N_PROTOCOL = process.env.N8N_PROTOCOL || 'http';
const N8N_USER = process.env.N8N_USER || 'admin';
const N8N_PASSWORD = process.env.N8N_PASSWORD || 'cahier-des-charges-migrator';
const AUTH_STRING = Buffer.from(`${N8N_USER}:${N8N_PASSWORD}`).toString('base64');

// Fonction pour faire une requ√™te HTTP
function makeRequest(method, endpoint, data = null) {
  return new Promise((resolve, reject) => {
    const url = `${N8N_PROTOCOL}://${N8N_HOST}:${N8N_PORT}${endpoint}`;
    const options = {
      method,
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Basic ${AUTH_STRING}`
      }
    };

    const req = (N8N_PROTOCOL === 'https' ? https : http).request(url, options, (res) => {
      let responseData = '';

      res.on('data', (chunk) => {
        responseData += chunk;
      });

      res.on('end', () => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          try {
            resolve(JSON.parse(responseData));
          } catch (e) {
            resolve(responseData);
          }
        } else {
          console.log(`Erreur HTTP ${res.statusCode}: ${responseData}`);
          reject(new Error(`HTTP ${res.statusCode}: ${responseData}`));
        }
      });
    });

    req.on('error', (error) => {
      reject(error);
    });

    if (data) {
      req.write(typeof data === 'string' ? data : JSON.stringify(data));
    }

    req.end();
  });
}

// Fonction pour cr√©er un workflow
async function createWorkflow(workflow) {
  try {
    console.log(`üîÑ Importation du workflow: ${workflow.name || workflow.id}`);
    
    // Formater les donn√©es pour l'API n8n
    const data = {
      name: workflow.name || `Workflow ${workflow.id}`,
      nodes: workflow.nodes || [],
      connections: workflow.connections || {},
      active: workflow.active === true,
      settings: workflow.settings || {},
      tags: workflow.tags || []
    };

    // V√©rifier si le workflow existe d√©j√†
    let existingWorkflows;
    try {
      existingWorkflows = await makeRequest('GET', '/rest/workflows');
    } catch (error) {
      console.log(`‚ö†Ô∏è Erreur lors de la r√©cup√©ration des workflows existants: ${error.message}`);
      existingWorkflows = { data: [] };
    }
    
    const existingWorkflow = (existingWorkflows.data || []).find(w => 
      w.name === data.name || (workflow.id && w.name.includes(workflow.id))
    );

    if (existingWorkflow) {
      console.log(`üìù Mise √† jour du workflow existant: ${data.name}`);
      await makeRequest('PUT', `/rest/workflows/${existingWorkflow.id}`, data);
      return { id: existingWorkflow.id, name: data.name };
    } else {
      console.log(`üìù Cr√©ation d'un nouveau workflow: ${data.name}`);
      const result = await makeRequest('POST', '/rest/workflows', data);
      return { id: result.id, name: data.name };
    }
  } catch (error) {
    console.error(`‚ùå Erreur lors de l'importation du workflow ${workflow.name || workflow.id}: ${error.message}`);
    return null;
  }
}

// Fonction principale
async function main() {
  try {
    console.log('üì• Importation des workflows dans n8n...');
    
    // Lire le fichier principal du pipeline
    const pipelineFile = path.resolve(__dirname, 'n8n.pipeline.clean.json');
    if (!fs.existsSync(pipelineFile)) {
      throw new Error(`Fichier ${pipelineFile} non trouv√©`);
    }
    
    const pipelineData = JSON.parse(fs.readFileSync(pipelineFile, 'utf8'));
    
    // Importer les workflows du pipeline principal
    const importedWorkflows = [];
    if (pipelineData.workflows && Array.isArray(pipelineData.workflows)) {
      for (const workflow of pipelineData.workflows) {
        const result = await createWorkflow(workflow);
        if (result) {
          importedWorkflows.push(result);
        }
      }
    }
    
    // Lire et importer les workflows depuis le dossier workflows
    const workflowsDir = path.resolve(__dirname, 'workflows');
    if (fs.existsSync(workflowsDir)) {
      const files = fs.readdirSync(workflowsDir).filter(f => f.endsWith('.json'));
      for (const file of files) {
        const filePath = path.join(workflowsDir, file);
        console.log(`üîÑ Traitement du fichier workflow: ${filePath}`);
        
        try {
          const workflowData = JSON.parse(fs.readFileSync(filePath, 'utf8'));
          const result = await createWorkflow(workflowData);
          if (result) {
            importedWorkflows.push(result);
          }
        } catch (error) {
          console.error(`‚ùå Erreur lors du traitement du fichier ${file}: ${error.message}`);
        }
      }
    }
    
    console.log('‚úÖ Importation des workflows termin√©e');
    console.log(`üìä ${importedWorkflows.length} workflows import√©s:`);
    importedWorkflows.forEach(w => console.log(`   - ${w.name} (ID: ${w.id})`));
    
    // √âcrire les IDs des workflows dans un fichier pour r√©f√©rence future
    fs.writeFileSync('workflow-ids.json', JSON.stringify(importedWorkflows, null, 2));
    
    return importedWorkflows;
  } catch (error) {
    console.error(`‚ùå Erreur: ${error.message}`);
    process.exit(1);
  }
}

// Ex√©cuter le script
main();
EOF

# Rendre le script ex√©cutable
chmod +x import-workflows.js

# Ex√©cuter le script d'importation des workflows
echo "üì• Importation des workflows dans n8n..."
node import-workflows.js

# V√©rifier si l'importation a r√©ussi
if [ $? -ne 0 ]; then
  echo "‚ùå Erreur lors de l'importation des workflows dans n8n."
  exit 1
fi

# Cr√©er un script pour ex√©cuter le pipeline de migration
cat > run-pipeline.js << 'EOF'
#!/usr/bin/env node

const fs = require('fs');
const path = require('path');
const https = require('https');
const http = require('http');

// Configuration
const N8N_HOST = process.env.N8N_HOST || 'localhost';
const N8N_PORT = process.env.N8N_PORT || '5678';
const N8N_PROTOCOL = process.env.N8N_PROTOCOL || 'http';
const N8N_USER = process.env.N8N_USER || 'admin';
const N8N_PASSWORD = process.env.N8N_PASSWORD || 'cahier-des-charges-migrator';
const AUTH_STRING = Buffer.from(`${N8N_USER}:${N8N_PASSWORD}`).toString('base64');

// Fonction pour faire une requ√™te HTTP
function makeRequest(method, endpoint, data = null) {
  return new Promise((resolve, reject) => {
    const url = `${N8N_PROTOCOL}://${N8N_HOST}:${N8N_PORT}${endpoint}`;
    const options = {
      method,
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Basic ${AUTH_STRING}`
      }
    };

    const req = (N8N_PROTOCOL === 'https' ? https : http).request(url, options, (res) => {
      let responseData = '';

      res.on('data', (chunk) => {
        responseData += chunk;
      });

      res.on('end', () => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          try {
            resolve(JSON.parse(responseData));
          } catch (e) {
            resolve(responseData);
          }
        } else {
          console.log(`Erreur HTTP ${res.statusCode}: ${responseData}`);
          reject(new Error(`HTTP ${res.statusCode}: ${responseData}`));
        }
      });
    });

    req.on('error', (error) => {
      reject(error);
    });

    if (data) {
      req.write(typeof data === 'string' ? data : JSON.stringify(data));
    }

    req.end();
  });
}

// Ex√©cuter un workflow
async function executeWorkflow(workflowId, runData = {}) {
  try {
    console.log(`üöÄ Ex√©cution du workflow (ID: ${workflowId})...`);
    
    const data = {
      workflowData: { id: workflowId },
      runData
    };
    
    const result = await makeRequest('POST', `/rest/workflows/${workflowId}/execute`, data);
    return result;
  } catch (error) {
    console.error(`‚ùå Erreur lors de l'ex√©cution du workflow: ${error.message}`);
    return null;
  }
}

// V√©rifier l'√©tat d'une ex√©cution
async function checkExecutionStatus(executionId) {
  try {
    console.log(`üîç V√©rification de l'√©tat de l'ex√©cution (ID: ${executionId})...`);
    
    const result = await makeRequest('GET', `/rest/executions/${executionId}`);
    return result;
  } catch (error) {
    console.error(`‚ùå Erreur lors de la v√©rification de l'√©tat: ${error.message}`);
    return null;
  }
}

// Attendre la fin d'une ex√©cution
async function waitForExecution(executionId, timeout = 300000, interval = 5000) {
  console.log(`‚è≥ Attente de la fin de l'ex√©cution (ID: ${executionId})...`);
  
  const startTime = Date.now();
  
  while (Date.now() - startTime < timeout) {
    const status = await checkExecutionStatus(executionId);
    
    if (!status) {
      console.warn(`‚ö†Ô∏è Impossible de r√©cup√©rer l'√©tat de l'ex√©cution ${executionId}`);
      return null;
    }
    
    if (status.finished) {
      console.log(`‚úÖ Ex√©cution ${executionId} termin√©e avec statut: ${status.status}`);
      return status;
    }
    
    // Attendre avant la prochaine v√©rification
    await new Promise(resolve => setTimeout(resolve, interval));
  }
  
  console.warn(`‚ö†Ô∏è D√©lai d'attente d√©pass√© pour l'ex√©cution ${executionId}`);
  return null;
}

// Fonction principale
async function main() {
  try {
    console.log('üöÄ Ex√©cution du pipeline de migration...');
    
    // Lire le fichier de configuration de migration
    const configFile = path.resolve(__dirname, 'migration-config.json');
    if (!fs.existsSync(configFile)) {
      throw new Error(`Fichier de configuration ${configFile} non trouv√©`);
    }
    
    const config = JSON.parse(fs.readFileSync(configFile, 'utf8'));
    
    // Lire les IDs des workflows import√©s
    const workflowIdsFile = path.resolve(__dirname, 'workflow-ids.json');
    if (!fs.existsSync(workflowIdsFile)) {
      throw new Error(`Fichier des IDs de workflow ${workflowIdsFile} non trouv√©`);
    }
    
    const workflowIds = JSON.parse(fs.readFileSync(workflowIdsFile, 'utf8'));
    
    // Ex√©cuter les √©tapes de migration
    const results = [];
    for (const step of config.migrationSteps) {
      console.log(`\nüìã √âtape: ${step.name}`);
      console.log(`   ${step.description}`);
      
      // D√©terminer les workflows √† ex√©cuter pour cette √©tape
      const stepWorkflows = [];
      
      // Correspondance bas√©e sur les agents
      for (const agent of step.agents) {
        const matchingWorkflows = workflowIds.filter(w => 
          w.name.toLowerCase().includes(agent.split('/').pop().toLowerCase())
        );
        
        for (const workflow of matchingWorkflows) {
          if (!stepWorkflows.some(w => w.id === workflow.id)) {
            stepWorkflows.push(workflow);
          }
        }
      }
      
      // Si aucun workflow n'est trouv√©, utiliser le workflow "php-analyzer" par d√©faut
      if (stepWorkflows.length === 0) {
        const defaultWorkflow = workflowIds.find(w => 
          w.name.toLowerCase().includes('php') && w.name.toLowerCase().includes('analyzer')
        );
        
        if (defaultWorkflow) {
          stepWorkflows.push(defaultWorkflow);
        }
      }
      
      console.log(`   Workflows √† ex√©cuter: ${stepWorkflows.map(w => w.name).join(', ') || 'Aucun'}`);
      
      // Ex√©cuter chaque workflow pour cette √©tape
      for (const workflow of stepWorkflows) {
        console.log(`\nüîÑ Ex√©cution du workflow: ${workflow.name}`);
        
        const runData = {
          step: step.name,
          agents: step.agents,
          sourcePath: config.cahierDesChargesPath || '/workspaces/cahier-des-charge/docs/cahier-des-charges',
          targetPath: config.outputDir || './reports',
          options: {
            parallel: config.parallel || false,
            verbose: config.verbose || false,
            autoFix: config.autoFix || false,
            validateWithCahierDesCharges: config.validateWithCahierDesCharges || true
          }
        };
        
        const execution = await executeWorkflow(workflow.id, runData);
        
        if (execution && execution.executionId) {
          const status = await waitForExecution(execution.executionId);
          results.push({
            step: step.name,
            workflow: workflow.name,
            executionId: execution.executionId,
            status: status ? status.status : 'unknown'
          });
        } else {
          console.warn(`‚ö†Ô∏è L'ex√©cution du workflow ${workflow.name} a √©chou√©`);
          results.push({
            step: step.name,
            workflow: workflow.name,
            status: 'failed_to_start'
          });
        }
      }
    }
    
    // Afficher le r√©sum√©
    console.log('\nüìä R√©sum√© du pipeline de migration:');
    for (const result of results) {
      const statusEmoji = result.status === 'success' ? '‚úÖ' : 
                         result.status === 'error' ? '‚ùå' : '‚ö†Ô∏è';
      
      console.log(`${statusEmoji} √âtape: ${result.step}`);
      console.log(`   Workflow: ${result.workflow}`);
      console.log(`   Statut: ${result.status}`);
      if (result.executionId) {
        console.log(`   ID d'ex√©cution: ${result.executionId}`);
        console.log(`   D√©tails: http://localhost:5678/execution/${result.executionId}`);
      }
      console.log('');
    }
    
    // √âcrire les r√©sultats dans un fichier
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const resultFile = `migration-results-${timestamp}.json`;
    fs.writeFileSync(resultFile, JSON.stringify(results, null, 2));
    
    console.log(`‚úÖ Pipeline de migration termin√©. R√©sultats enregistr√©s dans ${resultFile}`);
  } catch (error) {
    console.error(`‚ùå Erreur: ${error.message}`);
    process.exit(1);
  }
}

// Ex√©cuter le script
main();
EOF

# Rendre le script ex√©cutable
chmod +x run-pipeline.js

echo "üöÄ Ex√©cution du pipeline de migration..."
node run-pipeline.js

echo "‚úÖ Pipeline de migration termin√©!"